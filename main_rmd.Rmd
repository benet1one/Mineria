---
title: "Song Popularity Prediction"
subtitle: "An Analysis Using Data Mining Techniques"
author: |
  Garcia Garcia, Bernat  
  Maria Montés, Iker  
  Rota, Davide  
  Tobella Jacomet, Pol  
  
date: "2025-11-02"
output:
  pdf_document: 
    latex_engine: xelatex
header-includes:
- \usepackage{graphicx}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{}
- \fancyhead[L]{}
- \fancyhead[C]{\includegraphics[height=1.5cm]{Logos.png}}
- \fancyfoot[R]{}
- \fancyfoot[C]{\thepage}
- \renewcommand{\headrulewidth}{0.7pt}
- \setlength{\headheight}{48pt}
- \addtolength{\topmargin}{-25pt}
- \usepackage{titling}
- \usepackage{graphicx}
- \pretitle{\vspace{3cm}\begin{center}\LARGE}
- \posttitle{\vspace{0.5cm}\end{center}}
- \preauthor{\begin{center}\large}
- \postauthor{\end{center}}
- \predate{\begin{center}}
- \postdate{\vspace{3cm}\end{center}\begin{center}\includegraphics[width=8cm]{Logos.png}\end{center}}
---

\newpage
\tableofcontents
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE,message = FALSE)
options(tinytex.verbose = TRUE)
```

```{r}
#### Libraries that might be useful  ####

# library(clustMixType)
# library(tidyr)
# library(dplyr)
# library(visdat)
# library(GGally)
# library(ggplot2)
# library(ggpubr)
# library(purrr)
# library(corrplot)
# library(DMwR2)
# library(knitr)
# library(tinytex)
# library(cowplot)
# library(cluster)
# library(fpc)
# library(factoextra)
# library(dendextend)
# library(grid)
# library(sjPlot)
# library(inspectdf)
# library(isotree)
# library(reticulate)
# library(lubridate)
# library(NbClust)
# library(FactoMineR)
# library(modeest)
# library(FactoClass)
# library(broom)
# library(gridExtra)
```

# Introduction

In this document we will analyze data related to Spotify tracks, with the objective of determining a way to predict a song/track popularity in the site through information related to that track. ***Needs to be expanded a bit probably***
The purpose of this document is to identify an effective method for the prediction of the popularity of songs or tracks on Spotify, using the available information for each item. This analysis relies on track-related data and employs different techniques to construct a synthetic indicator of popularity. The goal is to uncover potential patterns and factors that explain why certain songs become more popular than others.


## Link to Repository

https://benet1one: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@github.com/benet1one/Mineria


# Motivation

Our objective is to develop a regression model capable of predicting the popularity of songs using musical attributes and associated metadata. This project aims to identify patterns and characteristics that explain why certain songs achieve higher levels of popularity, enabling streaming platforms to make more informed decisions regarding recommendations and playlist curation.

Specifically, the challenge consists of building a model that minimizes the Mean Absolute Percentage Error (MAPE), providing accurate predictions of each song's popularity (song_popularity). Additionally, we aim to analyze which musical attributes have the greatest influence on song popularity, in order to extract valuable insights for marketing and promotion strategies in the music industry.

In addition to the main objective, we seek to evaluate the quality and consistency of the provided data to ensure that the information used for modeling is reliable. An exploratory data analysis (EDA) will be conducted to select the most relevant variables and detect potential issues, such as missing values or outliers. Finally, we aim to propose recommendations for improving future predictions, either through the incorporation of new variables or through adjustments in the modeling methodology.

To assess model performance, we will primarily use control metrics such as MAPE, and complementary metrics such as RMSE and MAE. In addition, business-oriented indicators will be considered, such as the ability to identify potential hit songs before they achieve success on streaming platforms.

External Supporting sources:
- Kaggle competition: https://www.kaggle.com/competitions/prediccion-de-la-popularidad-de-canciones/overview
- Spotify API Documentation about musical attributes: https://developer.spotify.com/documentation/web-api

# Metadata

Our dataset consists of 13186 imputs of spotify tracks with 15 variables. From those, 3 are qualitative variables while 12 are quantitative variables.

***Probably some more explanation needed***
Our dataset consists of 13186 imput tracks, each one described by 15 variables. Among those, 3 are qualitative variables, so the provide a categorical information, while the other 12 are quantitative (or numerical) and allow to catch the measurable characteristics of the tracks.
The combination of all these components allow an analysis of factor that can result useful for the prediction of songs' popularity.

## Data source

The data has been obtained from the file *"train.csv"*. This file contains information related to tracks of Spotify, openly shared on the net. The data originally comes from the platform Kaggle, specifically from the contest *Predicción de la Popularidad de Canciones*.  

*Disclaimer: This data will not be used to train machine learning or AI models, as per the Policy Note of Spotify.*

\newpage
# Overview of the Data

## Qualitative variables

· **Audio mode**: Mode indicates the modality (*major or minor*) of a track, the type of scale from which its melodic content is derived.
Major is represented by 1 and minor is 0.
Example: 0

· **Key**: The key the track is in.
Integers map to pitches using standard Pitch Class notation.
E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on.
If no key was detected, the value is -1.
Range: -1..11 Example: 9

· **Time signature**: An estimated time signature.
The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).
The time signature ranges from 3 to 7 indicating time signatures of "3/4", to "7/4".
Range: 3 - 7 Example: 4

## Quantitative variables

### Relative/Ratio variables (0..1)

· **Liveness**: Detects the presence of an audience in the recording.
Higher liveness values represent an increased probability that the track was performed live.
*A value above 0.8 provides strong likelihood that the track is live.* Example: 0.0866

· **Danceability**: Danceability describes how suitable a track is for dancing *based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity*.
A value of 0.0 is least danceable and 1.0 is most danceable.
Example: 0.585

· **Audio valence**: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track.
*Tracks with high valence sound more positive* (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).
Example: 0.428

· **Energy**: Energy is a measure from 0.0 to 1.0 and represents a perceptual *measure of intensity and activity*.
Typically, energetic tracks feel *fast, loud, and noisy*.
For example, death metal has high energy, while a Bach prelude scores low on the scale.
Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.
Example: 0.842

· **Acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic.
*1.0 represents high confidence the track is acoustic*.
Example: 0.00242

· **Speechiness**: Speechiness detects the presence of *spoken words in a track*.
The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value.
*Values above 0.66 describe tracks that are probably made entirely of spoken words*.
Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music.
Values below 0.33 most likely represent music and other non-speech-like tracks.
Example: 0.0556

· **Instrumentalness**: Predicts whether a track contains no vocals.
"Ooh" and "aah" sounds are treated as instrumental in this context.
Rap or spoken word tracks are clearly "vocal".
*The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content*.
Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.
Example: 0.00686

· **Song popularity**: Objective variable, which describes the popularity of the song.
The popularity of a track is a value between 0 and 100, with 100 being the most popular.
Example: 69


\newpage
### Absolute variables

· **ID**: A count, from 1 to 13186 ***Not spotify ID***

· **Loudness**: The overall loudness of a track in decibels (dB).
Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.
Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude).
*Values typically range between -60 and 0 db*.
Example: -5.883

· **Song duration (ms)**: The duration of the track in milliseconds.
Example: 237040

· **Tempo**: The overall estimated tempo of a track in beats per minute (BPM).
In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
Example: 118.211


## The data

In this section we show a summary of our data
```{r}
source("reading.R")
```

# Methodology

The project follows a structured approach based on CRISP-DM (Cross Industry Standard Process of Data Mining), adapted to the context of predicting song popularity. Each phase includes assigned team members to ensure an efficient and transparent workflow.

## Business Understanding

In this initial phase, the primary objective is to understand the problem domain, define clear goals for the model, and establish the evaluation metrics that guide the project. Key activities include identifying the target variables (song_popularity), formulating the main research question, and determining the secondary objectives, such as identifying the most influential musical features.

%%% is responsable for this phase, coordinating the team in defining KPIs and the SMART criteria (Specific, Measurable, Achievable, Relevant, Time-bound). Additionaly, this stage involves assessing avaible resources and constraints, ensuring that the team has access to the required data and computational tools.
Proper completion of this phase guarantees that subsequent stages are aligned with the project’s business objectives.

## Data Understanding

This phase focuses on collecting, exploring, and verifying the quality of the dataset. %%% lead the activities, ensuring that the data preparation process is both technically rigorous and aligned with the project’s objectives. Their tasks include:

Gathering data from the training file (train.csv) and ensuring it contains the necessary attributes for modeling.

Performing exploratory data analysis (EDA) to identify patterns, distributions, correlations, and potential anomalies.

Checking for missing values, outliers, and duplicates to ensure the dataset is suitable for modeling.

Structuring and integrating data as needed, for instance, creating derived features or harmonizing formats across variables.

The goal of this phase is to obtain a comprehensive understanding of the dataset and identify potential issues that may affect the modeling process.

## Data Preparation

Once the data has been understood, this phase aims to construct the dataset that will be used for modeling. %%% are responsible for executing the preprocessing steps required to ensure data quality and consistency. Key tasks include:

- Cleaning the dataset by handling missing values and removing inconsistencies.
- Transforming variables (scaling, normalization, encoding categorical variables).
- Conducting feature engineering to create new meaningful predictors (e.g., ratios, combined variables, or interaction terms).
- Selecting the most relevant features to optimize model performance and prevent overfitting.
- Preparing final datasets for training, validation, and testing.

The output of this phase is a high-quality analytical base that will serve as input for model training and evaluation.

## Modeling

%%% is responsible for the modeling phase, which includes:

- Selecting appropriate algorithms for regression tasks, considering models such as K-Nearest Neighbors (KNN) and Naive Bayes as initial approaches.
- Applying Association Rules to uncover hidden relationships between musical attributes and popularity patterns.
- Evaluating models using the MAPE metric, as well as complementary metrics such as RMSE and MAE.
- Interpreting results to understand which musical variables have the greatest influence on a song’s popularity.

The modeling phase ensures that the predictive system is both accurate and robust, capable of generalizing well to unseen data.

## Evaluation

This phase assesses the model’s effectiveness in meeting the defined business objectives. %%% jointly lead the evaluation, verifying both the technical performance and the business relevance of the results. Key tasks include:

- Comparing model results against predefined KPIs and metrics (MAPE, RMSE, MAE).
- Conducting error analysis to identify potential biases or patterns of poor performance.
- Assessing model interpretability to ensure insights are understandable to stakeholders.

Validating whether the model meets the initial objectives defined during the Business Understanding phase.

The purpose of this phase is to ensure the chosen model not only performs well statistically but also provides valuable insights that align with business needs.

## Deployment

The Deployment phase, typically involving implementation of models into production environments or dashboards, is not part of the current project scope.

However, this phase is included conceptually within the CRISP-DM framework to illustrate the complete data science lifecycle.
In a future extension, deployment activities could involve automating model retraining, integrating predictions into a Spotify-like recommendation system, or visualizing results through interactive dashboards.


# Pre-Processing

The Exploratory Data Analysis revealed intricate, complex relationships between the variables.
Combined with our business understanding, we are able to carefully pick and calibrate the models
we want to use for this important step of the treatment.

Some univariate outliers were considered errors. A tempo of zero was treated as a missing
value. Two positive loudness values were imputed as zero, which seems to be a "soft maximum"
for the variable if we plot its distribution.

## Missing data

We were informed that the missing values were generated completely at random. Little's test
confirms there is no evidence for the contrary.

A very small amount of observations in the training set had less than half of the information, 
and we decided to eliminate them.

Imputation was carried in two steps:
- Direct imputation of `audio_mode` by using `key`.
- Simultaneous imputation of the rest with MICE.

By using empirical data, we created a variable `prob_major`, which represents 
$\Pr(\text{Audio Mode} = \text{Major}\ |\ Key)$. For the songs where key is known, we imputed
`audio_mode` as Major if `prob_major > 0.5`; and for the songs with unknown key, `audio_mode` was
imputed in the next step (MICE). We plan to use one of the two variables depending on the 
nature of the model. For example, a linear model can use `prob_major` as a regressor instead
of `audio_mode`, and naturally takes into account the uncertainty of the imputation.

Lastly, we impute the rest of the variables using MICE with a custom set of predictor models and
predictor matrix, tailored to the needs of each variable. Predictive Mean Matching (PMM) worked
well for most variables, but we also used Random Forests and Lasso Regressions to capture
relationships that PMM was not able to correctly predict.

We validated the imputation by checking if all distributions were kept similar, and all
known relationships between variables were respected. Two of the most difficult variables
to impute were acousticness and loudness, but the result is outstanding:

```{r}
source("Bernat/missing_validation.R")
imputation_scatterplot(loudness, acousticness)
```

## Outliers

Detecting outliers was not a straightforward task. Some values may seem extraordinary, but aren't.
A track that lasts 29 minutes looks like a mistake, but could very well be a classical piece. For this reason, we did not consider any univariate outliers and approached the problem from a multivariate
perspective.

We implemented a modified version of [Attribute wise learning for scoring outliers](https://cienciadedatos.net/documentos/67_deteccion_anomalias_also) (ALSO), which
uses k-fold prediction, like the article suggests, to avoid predicting an outlier with a model
trained on that same outlier. This algorithm takes into account the strong dependencies between variables, something distance based methods like Local Outlier Factor (LOF) or Heriarchical Clustering 
do not directly use. 

Additionally, we detected some clear outliers in the scatterplot between instrumentalness
and speechiness. We manually weighted these with a custom formula. By combining the two,
we assigned a weight to each observation to be used in models.

```{r}
readRDS("data/songs_outlied.RDS") |> 
    ggplot(aes(x = instrumentalness, y = speechiness, color = outlier_weight)) +
    geom_point(size = 1.6) +
    scale_color_gradient(low = "black", high = "royalblue1") +
    theme_minimal()
```

\newpage
## KNN and Naive Bayes
In this step the aim is to find some algorithms for the regression task, considering two different strategies as initial approach.

#KNN
The first method applied is K-Nearest Neighbors (KNN) in a regression context.
Initially, the cleaned dataset was divided into a training and a test set, in order to obtain reliable results and assess the model’s ability to generalize to unseen data. The next step involved defining the features and the numerical target variable within these subsets.
To ensure good KNN performance, all features were converted to numerical format and standardized, since differences in scale could otherwise strongly affect the computation of distances. The model was then trained and used to make predictions, which were evaluated using one of the most appropriate KPIs for regression tasks, the Root Mean Squared Error (RMSE), along with the correlation between predicted and actual values.
Finally, the parameter k (number of neighbors) was optimized based on the RMSE results, with the best performance obtained for k = 5.
This analysis indicates that KNN can be a viable method for further exploration, although it performs satisfactorily only for specific values of k and is computationally expensive. Therefore, testing alternative models might be advisable for improved efficiency and scalability.

PLOT OF KPI/k

# Naive Bayes
At first glance, the use of the Naive Bayes algorithm appeared to be a forced approach for predicting song popularity. This mainly because Naive Bayes is designed for categorical target variables, whereas our target (song popularity) is continuous and numerical in nature.
To make the method applicable, we transformed the target variable into three discrete categories representing low, medium, and high popularity levels. However, after applying the Naive Bayes model, the results were unsatisfactory: the predictions were heavily biased toward the “high” category, leading to an imbalanced outcome. Furthermore, the accuracy derived from the corresponding confusion matrix was quite low, indicating that Naive Bayes is not well suited for this regression-oriented problem.
"Accuracy: 0.33333"
